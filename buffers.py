import collections
import numpy as np

# returns a namedtuple with (state, action, reward...)
Experience = collections.namedtuple("Experience", field_names=['state', 'action', 'reward', 'done', 'new_state'])


class ExperienceBuffer:
    """
    Buffer of fixed capacity to handle previous experiences for bootstrap sampling.
    """
    def __init__(self, capacity:int):
        self.buffer = collections.deque(maxlen=capacity)

    def __len__(self):
        return len(self.buffer)

    def append(self, experience:Experience):
        self.buffer.append(experience)

    # [self.buffer[idx] for idx in indices] is a list of [exp, exp,..., exp]
    # *[exp,exp,exp] returns then exp, exp, exp as arguments to a function
    # zip(exp, exp, exp) then rearranges the tuples such that new tuples are created
    # where the first elements of every exp tuple is in one tuple,
    # the second elements of every exp tuple are in one tuple etc
    # TODO standardize variable type declarations
    def sample(self, batch_size:int):
        """
        Sample random batch of experiences from buffer.
        :param batch_size: int
        :return: states, actions, rewards, dones, next_states
        """
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])
        return states, np.array(actions), np.array(rewards, dtype=np.float32), \
               np.array(dones, dtype=np.uint8), np.array(next_states)

    def fill(self, env, nsamples:int):
        '''
        fill the buffer with experiences from random actions generated by environment.
        :param env: gym environment
        :param start_size: int
        :return: None
        '''
        assert nsamples <= self.capacity, "Number of samples to fill is bigger than buffer size!"
        state = env.reset()
        print("Populating Buffer...")
        for frames in range(nsamples):
            action = env.action_space.sample()
            new_state, reward, is_done, _ = env.step(action)
            exp = Experience(state, action, reward, is_done, new_state)
            self.append(exp)
        print("Buffer populated!")
